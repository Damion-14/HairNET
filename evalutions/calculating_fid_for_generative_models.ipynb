{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "S83--6_AvvV_",
        "outputId": "933e14de-a7a9-4a86-e2c2-dec693620ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to stack",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3b549477e2ba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mreal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages_to_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/test images/10_random_images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages_to_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/test images/lora result imags'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3b549477e2ba>\u001b[0m in \u001b[0;36mimages_to_numpy_array\u001b[0;34m(directory_path, resize_shape)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# If images have different shapes and you haven't resized them, this step can fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mimages_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "# Load Inception model\n",
        "inception = models.inception_v3(pretrained=True).eval()\n",
        "inception.fc = nn.Identity()  # Remove last layer to get features\n",
        "\n",
        "def get_features(images, model, batch_size=32):\n",
        "    features = []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch = torch.from_numpy(images[i:i + batch_size])\n",
        "        batch = batch.permute(0, 3, 1, 2)\n",
        "        with torch.no_grad():\n",
        "            batch_features = model(batch).cpu().numpy()\n",
        "        features.append(batch_features)\n",
        "    return np.concatenate(features)\n",
        "\n",
        "def calculate_fid(real_features, generated_features):\n",
        "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
        "    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)\n",
        "    diff = mu1 - mu2\n",
        "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "\n",
        "def images_to_numpy_array(directory_path, resize_shape=None):\n",
        "    \"\"\"\n",
        "    Loads all images in directory_path into a single NumPy array.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    directory_path: str\n",
        "        Path to the directory containing the images.\n",
        "    resize_shape: (width, height) tuple or None\n",
        "        If not None, resize each image to this shape.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    np.ndarray\n",
        "        A NumPy array of shape (num_images, height, width, channels).\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify all image files in the directory\n",
        "    # You can adjust the glob pattern for jpg, png, etc.\n",
        "    image_files = glob.glob(os.path.join(directory_path, \"*.jpg\"))\n",
        "    image_files = glob.glob(os.path.join(directory_path, \"*.jpeg\"))\n",
        "    image_files += glob.glob(os.path.join(directory_path, \"*.png\"))\n",
        "    image_files += glob.glob(os.path.join(directory_path, \"*.webp\"))\n",
        "    images_list = []\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Open each image\n",
        "        with Image.open(image_file) as img:\n",
        "            # Optionally resize\n",
        "            img = img.resize((299,299))\n",
        "            # Convert to NumPy array (automatically channels=3 for RGB or channels=4 for RGBA, etc.)\n",
        "            img = img.convert('RGB')\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            images_list.append(img_array)\n",
        "\n",
        "    # Convert the list to a NumPy array\n",
        "    # Shape would be (N, height, width, channels)\n",
        "    # If images have different shapes and you haven't resized them, this step can fail\n",
        "    print(np.array(images_list).shape)\n",
        "    images_np = np.stack(images_list, axis=0)\n",
        "\n",
        "    return images_np\n",
        "real_images = images_to_numpy_array('/content/drive/MyDrive/Colab Notebooks/test images/10_random_images')\n",
        "generated_images = images_to_numpy_array('/content/drive/MyDrive/Colab Notebooks/test images/lora result imags')\n",
        "\n",
        "real_features = get_features(real_images, inception)\n",
        "generated_features = get_features(generated_images, inception)"
      ]
    },
    {
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "# Load Inception model\n",
        "inception = models.inception_v3(pretrained=True).eval()\n",
        "inception.fc = nn.Identity()  # Remove last layer to get features\n",
        "\n",
        "def get_features(images, model, batch_size=32):\n",
        "    features = []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch = torch.from_numpy(images[i:i + batch_size])\n",
        "        batch = batch.permute(0, 3, 1, 2)\n",
        "        with torch.no_grad():\n",
        "            batch_features = model(batch).cpu().numpy()\n",
        "        features.append(batch_features)\n",
        "    return np.concatenate(features)\n",
        "\n",
        "def calculate_fid(real_features, generated_features):\n",
        "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
        "    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)\n",
        "    diff = mu1 - mu2\n",
        "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "\n",
        "def images_to_numpy_array(directory_path, resize_shape=None):\n",
        "    \"\"\"\n",
        "    Loads all images in directory_path into a single NumPy array.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    directory_path: str\n",
        "        Path to the directory containing the images.\n",
        "    resize_shape: (width, height) tuple or None\n",
        "        If not None, resize each image to this shape.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    np.ndarray\n",
        "        A NumPy array of shape (num_images, height, width, channels).\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify all image files in the directory\n",
        "    # You can adjust the glob pattern for jpg, png, etc.\n",
        "    image_files = glob.glob(os.path.join(directory_path, \"*.jpg\"))\n",
        "    image_files.extend(glob.glob(os.path.join(directory_path, \"*.jpeg\"))) # Use extend instead of assignment to accumulate files\n",
        "    image_files.extend(glob.glob(os.path.join(directory_path, \"*.png\"))) # Use extend instead of assignment to accumulate files\n",
        "    image_files.extend(glob.glob(os.path.join(directory_path, \"*.webp\"))) # Use extend instead of assignment to accumulate files\n",
        "    images_list = []\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Open each image\n",
        "        with Image.open(image_file) as img:\n",
        "            # Optionally resize\n",
        "            img = img.resize((299,299))\n",
        "            # Convert to NumPy array (automatically channels=3 for RGB or channels=4 for RGBA, etc.)\n",
        "            img = img.convert('RGB')\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            images_list.append(img_array)\n",
        "\n",
        "    # Convert the list to a NumPy array\n",
        "    # Shape would be (N, height, width, channels)\n",
        "    # If images have different shapes and you haven't resized them, this step can fail\n",
        "    print(np.array(images_list).shape)\n",
        "\n",
        "    # Check if images_list is empty before stacking\n",
        "    if not images_list:\n",
        "        raise ValueError(f\"No images found in directory: {directory_path}. Please check the directory path and file extensions.\")\n",
        "\n",
        "    images_np = np.stack(images_list, axis=0)\n",
        "\n",
        "    return images_np\n",
        "real_images = images_to_numpy_array('/content/drive/MyDrive/539/ECE539/CelebAMask-HQ_500/data/images')\n",
        "generated_images = images_to_numpy_array('/content/drive/MyDrive/539/ECE539/inpaint_lora/image results')\n",
        "\n",
        "real_features = get_features(real_images, inception)\n",
        "generated_features = get_features(generated_images, inception)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wEruiXAI9fD",
        "outputId": "67bdce9e-a431-4c47-8507-3218eee783b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:01<00:00, 72.2MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(492, 299, 299, 3)\n",
            "(360, 299, 299, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_fid(real_features, generated_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8bsITpf-OMz",
        "outputId": "00fb4189-83cb-457b-d2c0-1dca8a1dbe4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(282.6513325250974)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r5pTeKC1m9pk",
        "outputId": "63a5bbb4-8442-466f-daef-f055a77155a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0Hllqh_GFrv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}